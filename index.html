<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <title>Nick Doiron - ML Portfolio</title>
    <link rel="stylesheet" href="//stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous"/>
    <style>
.center {
  text-align: center;
}
.center img {
  margin-left: auto;
  margin-right: auto;
  display: block;
  max-width: 90%;
}
.row:nth-child(even) {
  background: #eee;
}
h4 {
  margin-top: 16px;
  margin-left: -20px;
}
li {
  margin-bottom: 8px;
}
    </style>
  </head>
  <body>
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <br/><br/>
          <h1>Nick Doiron / ML Portfolio
            <br/>
            <small style="font-size:40%">Contact <a href="mailto:ndoiron@mapmeld.com">ndoiron@mapmeld.com</a></small></h1>
          <p>
            No handwritten digits or irises here. Some interesting projects:
          </p>
          <hr/>
          <ul>
            <h4>Language</h4>
            <li>
              <strong>Training new BERT models</strong> -
              uploaded the first Transformers models for Hindi, Bengali, Tamil, and Dhivehi on HuggingFace; pretrained with Google ELECTRA
              on OSCAR CommonCrawl and latest Wikipedia. Finetuning showed better results than Multilingual BERT
              on movie review (regression), news topic (classification), and other tasks.
              <br/>
              Researchers and developers use these models in their work. The Bengali model was evaluated in the paper <a href="https://www.researchgate.net/profile/Partha_Chakraborty7/publication/346651608_Bangla_Documents_Classification_using_Transformer_Based_Deep_Learning_Models/links/5fcbc45c45851568d142a00e/Bangla-Documents-Classification-using-Transformer-Based-Deep-Learning-Models.pdf">Bangla Documents Classification using Transformer
Based Deep Learning Models</a>, presented at the 2nd International Conference on Sustainable Technologies for Industry 4.0, in Dhaka.
              This Hindi model was evaluated in the paper <a href="https://arxiv.org/abs/2101.05494">Hostility Detection in Hindi leveraging Pre-Trained Language Models</a>,
              tackling the shared task at CONSTRAINT 2021.
              <br/>
              This model was also recommended <a href="https://github.com/explosion/spaCy/blob/78d6ff4dd4ae5a8f78db4e7d7f5b8c24dba94ed5/spacy/cli/templates/quickstart_training_recommendations.yml#L103">by the spaCy docs</a>
              until I <a href="https://github.com/explosion/spaCy/issues/7044">suggested</a> a newer, more accurate model from Indian institution AI4Bharat.
              <!--
              <a href="https://medium.com/@mapmeld/a-whole-world-of-bert-f20d6bd47b2f">Why monolingual models?</a> -
              <a href="https://medium.com/@mapmeld/teaching-hindi-to-electra-b11084baab81">blog post</a>
              - <a href="https://colab.research.google.com/drive/1GngBFn_Ge5Hd2XI2febBhZyU7GDiqw5w">Tamil pretraining CoLab</a>
              - <a href="https://github.com/mapmeld/hindi-bert/blob/latest/HindiMovieReviews.ipynb">SimpleTransformers vs. finetuned ELECTRA</a> - <a href="https://github.com/mapmeld/hindi-bert/blob/latest/HindiMovieReviews_HF.ipynb">Transformers benchmark</a>
-->
            </li>
            <li>
              <strong>Gender bias in Spanish BERT</strong> -
              Adapted Word Embedding Association Tests for Spanish (parallel masculine and feminine word lists), and 
              made a script to mirror sentences (el actor británico <-> la actriz británica). The outputs can be used for
              counterfactuals or for data augmentation, measuring fairness and improving accuracy.
              <br/>
              <a href="https://medium.com/p/gender-bias-in-spanish-bert-1f4d76780617?source=email-f6542c4481e2--writer.postDistributed&sk=179de4323be6e07613aeea8e0b4ae14d">blog post and notebooks</a>
              - <a href="https://github.com/MonsoonNLP/WEAT-ES">WEAT-ES</a> - <a href="https://huggingface.co/monsoon-nlp/es-seq2seq-gender-decoder">seq2seq neural model</a>
            </li>
            <li>
              <strong>Arabic NLP</strong> -
              <a href="https://medium.com/voice-tech-podcast/sentiment-analysis-in-arabic-f08cd20a76d">Comparing sentiment analysis libraries</a>,
              training predictive models on
              <a href="https://medium.com/@mapmeld/embeddings-for-arabic-dialect-classification-1cf84f336044">multiple Arabic dialect datasets</a>
              (<a href="https://gist.github.com/mapmeld/aa17c987174ce280522a03f377760650">notebook</a>),
              and developing a <a href="https://colab.research.google.com/drive/1TuDfnV2gQ-WsDtHkF52jbn699bk6vJZV?usp=sharing">seq2seq counterfactual model</a>
              <br/>
              My <a href="https://huggingface.co/monsoon-nlp/sanaa-dialect">dialect-controlling GPT-2 model</a> was cited in
              the <a href="https://arxiv.org/abs/2012.15520">AraGPT2</a> paper by researchers at the American University of Beirut.
            </li>
            <li>
              <strong>Esperanto LSTM</strong> -
              trained on a Wikipedia corpus; generated text was nonsensical, but grammatical correctness was ultimately useful for editing Wikipedia
              <br/>
              <a href="https://medium.com/@mapmeld/esperanto-nlp-part-3-correcting-grammar-6d34f93bded1">blog post</a> - <a href="https://github.com/mapmeld/tensorflow-rnn-esperanto">code</a>
            </li>
            <div class="center">
              <img src="./grammar_check.png"/>
            </div>
            <h4>Privacy in ML</h4>
            <li>
              <strong>No Phone GPT-2</strong> -
              swapped number tokens and moved embeddings such that memorized US phone numbers and associated personal information are nullified.
              This is the first step toward removing more complex pieces of information (such as addresses and calendar dates).
              This was
              shared on <a href="https://blog.openmined.org/scrambling-memorized-info-in-gpt-2/">the OpenMined blog</a>.
            </li>            
            <h4>Social Media</h4>
            <li>
              <strong>AOC Reply Dataset</strong> -
              scraped 110k Twitter replies to Congresswoman @AOC, explored dataset with weakly supervised troll detection,
              first with Google AutoML, then later with
              <a href="https://medium.com/@mapmeld/deciphering-explainable-ai-with-eli5-22c90a06a32a">SciKit-Learn</a>
              and
              <a href="https://medium.com/@mapmeld/deciphering-explainable-ai-with-gpt-2-528611a3c75">GPT-2</a>
              . I continue to collaborate with researchers who analyze
              this dataset.
              <br/>
              <a href="https://github.com/mapmeld/aoc_reply_dataset/tree/master/all_tweets">dataset</a>
              - <a href="https://towardsdatascience.com/the-aoc-reply-dataset-190925c3d6f9">blog post</a>
            </li>
            <li>
              <strong>DeepClapback</strong> -
              downloaded Reddit comments into PostgreSQL to find short replies which greatly outscored their parent (for example: "[citation needed]")
              and predict the best response to any post.
              Used Google AutoML.
              <br/>
              <a href="https://blog.goodaudience.com/can-deepclapback-learn-when-to-lol-e4a2092a8f2c">blog post</a>
            </li>
            <li>
              <strong>Data Engineering with Kedro</strong> -
              re-labeling Twitter disinfo data releases by language, using data pipeline library Kedro with new visualizations (see below)
              <br/>
              <a href="https://medium.com/@mapmeld/transparent-data-flow-with-kedro-eba842de4eb2">blog post</a> - <a href="https://github.com/mapmeld/kedro-log-viz">code</a>
            </li>
            <div class="center">
              <img src="./lang_split.png"/>
            </div>
            <h4>Tabular Data</h4>
            <li>
              <strong>Student Dropout Contract</strong> -
              processed 11 million rows of student records into a PostGIS database on contract with the Inter-American Development Bank.
              <br/>
              Developed a data visualization (map and dashboard).
              SciKit-Learn (Bayesian Ridge, XGBoost) to model dropout rates, crime rates, and nearby geography (from OpenStreetMap), and school stats.
              Used ELI5 to measure significance of each column.<br/>
              Dropout rate was not very predictable, especially in smaller rural schools (where one student leaving = 20% dropout rate),
              but we found a key point is when students must change schools to enter 7th grade.
              <br/>
              <a href="https://github.com/mapmeld/stay-in-school">code</a>
            </li>
            <li>
              <strong>Airbnb Price Prediction with AutoKeras</strong> -
              Trained a model using Airbnb prices and OpenStreetMap local data. Used Uber's Manifold to visualize the significance of each column (see below).
              <br/>
              <a href="https://medium.com/@mapmeld/geodata-and-airbnb-price-predictions-e94dc5b5c150">blog post</a> - <a href="https://colab.research.google.com/drive/1WayFrrQGKJWBUNmLY4E37woMVC1D-k3q">notebook</a>
            </li>
            <div class="center">
              <img src="./feature_weights.png"/>
            </div>
        <!--
            <h4>Object Detection</h4>
            <li>
              <strong>Finding errors in self-driving car dataset</strong> -
              based on <a href="https://blog.roboflow.ai/self-driving-car-dataset-missing-pedestrians/">this Computer Vision project</a>, but instead of repairing Udacity's dataset manually, I compared
              results from the original dataset, YOLO out-of-the-box, and (to-be-completed) a finetuned model.
              <br/>
              <a href="https://colab.research.google.com/drive/14O0guWGnY9F0Bg7ANLt7_YZ4Y0ZYQRJX">notebook</a> - <a href="https://medium.com/@mapmeld/cleaning-up-a-self-driving-car-dataset-1303ebf8c7a7">blog post</a>
            </li>
            <div class="center">
              <img src="./detection.png"/>
              I scanned for frames where pedestrians were unlabeled; YOLOv3 detected mostly false positives, but also noticed this motorcyclist who was missing from Udacity/CrowdAI dataset
            </div>
-->
            <!-- <h4>Serving Models</h4>
            <li>
              https://medium.com/@mapmeld/crud-app-for-text-classification-a63c8c52b0ae
              https://medium.com/@mapmeld/finalizing-the-crud-ml-app-5b1a9320a54b
            </li> -->
            <!-- <h4>GANs</h4>
            <li>
              On the IADB contract, I tried <a href="https://medium.com/@mapmeld/anonymizing-data-with-gans-75bd188fa9c6">anonymizing tablar data</a>
              for data releases.
              <br/>
              I generated faces from the Facebook DeepFake challenge dataset - <a href="https://medium.com/analytics-vidhya/exploring-the-deepfake-detection-challenge-e926488791f5">blog post</a> - <a href="https://colab.research.google.com/drive/1d_SGnSfeKQvPZqw-Hz-v50QhF-LHehUQ">notebook</a>
              <br/>
              though later I learned DeepFakes are usually created with autoencoders 🤷.
            </li>
            <div class="center">
              <img src="./deepfake.jpeg"/>
            </div> -->
            <h4>Non-ML Coding</h4>
            I'm a full-stack developer with geospatial experience. Previously I worked in a Software Engineer / Data Scientist role at McKinsey & Company, and
            as an open data expert for Code for America, the City of Boston, ESRI, and the Asia Foundation.
            <br/>
            <a href="https://github.com/mapmeld/">GitHub</a>: I've contributed to
            <a href="https://github.com/mapmeld/fortran-machine">Fortran.IO</a>,
            <a href="https://github.com/keras-team/autokeras/pull/1107">AutoKeras examples</a>,
            <a href="https://github.com/openstreetmap/iD">OpenStreetMap (osm.org and iD Editor)</a>,
            <a href="https://github.com/nextstrain/auspice">NextStrain</a>,
            <a href="https://github.com/mapmeld/quantum-peep">a quantum computing library</a>,
            and <a href="https://github.com/MetaMask/metamask-extension">a cryptocurrency wallet</a>.
            <!-- <h4>Text-to-Speech</h4>
            <li>
              A failed attempt at <a href="https://medium.com/@mapmeld/attempting-isixhosa-text-to-speech-c148750b85bf">isiXhosa</a> -
              Mozilla TTS is a good library, but I would need to write a new phonemizer to break up the source text, and more GPU time to train the model.
              Used AWS Sagemaker for more powerful GPUs.
            </li> -->
            <h4>Writing and Outreach</h4>
              In 2020, I participated in an <strong>AI and International Law</strong> workshop at the Asser Instituut in The Hague.
              It was a good view of AI/ML from a legal and ethical perspective:
              <a href="https://medium.com/@mapmeld/discussing-ai-in-the-hague-67432267f342">blog post</a>.
            <br/>
            In 2021 my feedback led to a small edit and acknowledgement in the "Stochastic Parrots" AI Ethics paper.
            I support the full team behind this paper, who should all have been allowed credit on their work, and recognize their contributions are much more than my own.
              <hr/>
              In recent years I conducted data visualization and PostGIS workshops at PyCon India, PyCon Zimbabwe, and refugee code schools in Turkey and Iraqi Kurdistan.
              <!-- <a href="https://medium.com/@mapmeld/interesting-papers-from-ai-skeptics-edc97c0c39f7">Interesting Papers from AI Skeptics</a>
              <br/>
              <a href="https://medium.com/@mapmeld/near-future-predictions-for-machine-learning-97124c161b4d">Near-Future Predictions for Machine Learning</a>
              <br/>
              <a href="https://medium.com/@mapmeld/new-in-nlp-super-cool-projects-and-articles-dfc2c797c31e">New in NLP: Super Cool Projects and Articles</a> -->
              <!--
              https://medium.com/@mapmeld/what-if-explainable-ai-doesnt-work-749cc1f70281
              https://medium.com/swlh/whats-worth-doing-in-machine-learning-d65bf6ec8b60
            -->
            </li>
          </ul>
        </div>
      </div>
      <!-- <div class="row">
        <div class="col-sm-12">
          <h2>Steps to an Awesome Text Classifier</h2>

          <h3>Can a Text Classifier Help?</h3>
          <p>
            <strong>This page helps you get started building a model using SimpleTransformers.</strong>
            <br/>
            Text classifiers help humans separate out two or more groups of messages.
            <br/>
            Examples: spam or not-spam, liberal or conservative,
            positive / neutral / negative reviews, or dividing news stories into topics.
          </p>
          <p>
            <strong>If your project is</strong> translating, answering questions, making a chatbot, or summarizing/processing lots of text, those links are
            other tools for doing that.
          </p>
          <p>
            <strong>Some tasks are hard for people and computers</strong>: picking the most romantic poem,
            guessing if a review was written by someone from New York,
            grading students' creative writing.
            <br/>
            Machine learning works best when it's <em>an easy task for humans</em> (such as detecting spam)
            and <em>the computer will help by doing it faster</em>.
          </p>
        </div>
        <div class="col-sm-12">
          <h3>Picking a model in your language</h3>
          <p>
            For most cases you will start out with a language model created by researchers.
            They have trained neural networks and picked vectors for words based on a super-large dataset of text in your language.
          </p>
          <p>
            The leading models for English monolingual text are T5, XLM, and BERT.<br/>
          </p>
          <p>
            A full 100 languages are in Multilingual BERT and XLM-R. Cross-lingual
            training means a potential for transfer learning (training on one language
            can be effective on others).
          </p>
          <p>
            Transformers has other monolingual models which you can research here.
            I wrote about why we don't use a multilingual all the time at https://medium.com/@mapmeld/a-whole-world-of-bert-f20d6bd47b2f

            XLM-R large vs. base vs. BERT vs. DIY
Afrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanized, Bosnian, Breton, Bulgarian, Burmese, Burmese, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanized, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskri, Scottish, Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanized, Telugu, Telugu Romanized, Thai, Turkish, Ukrainian, Urdu, Urdu Romanized, Uyghur, Uzbek, Vietnamese, Welsh, Western, Frisian, Xhosa, Yiddish
https://github.com/VinAIResearch/PhoBERT
https://mdbootstrap.com/docs/jquery/forms/autocomplete/
http://vectors.nlpl.eu/repository/
https://bertlang.unibocconi.it/
            <a href="https://huggingface.co/models">source</a> - use this if you can't find it above
https://github.com/facebookresearch/XLM
https://github.com/google-research/bert/blob/master/multilingual.md
          </p>
          <p>
            If your language is not available, or it doesn't perform to your liking in your industry/vocabulary,
            you might need to do it yourself.
          </p>
        </div>
      </div> -->
    </div>
    <br/><br/><br/>
  </body>
</html>
